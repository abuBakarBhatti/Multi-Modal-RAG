# ğŸ§  Multimodal RAG (Retrieval-Augmented Generation) App

## Test commit to verify author change


This is a full-stack **Multimodal RAG** (Retrieval-Augmented Generation) application built using:

- âš™ï¸ **FastAPI** (Python) for the backend
- âš›ï¸ **React (TypeScript)** for the frontend
- ğŸ¤– **Multimodal LLMs** (like LLaMA 4 Maverick via Together API or custom local models via Ollama)

The app enables advanced question answering and document interaction using **multimodal large language models**.

---

## ğŸš€ Project Structure

```bash
your-project/
â”‚
â”œâ”€â”€ backend/               # FastAPI backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ frontend/              # React frontend (TypeScript)
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ README.md              # You're reading it!


ğŸ› ï¸ Getting Started
1. Backend Setup (FastAPI)
ğŸ”¹ Step 1: Create and activate a virtual environment

cd backend
python -m venv venv
source venv/bin/activate  
#Windows:venv\Scripts\activate

ğŸ”¹ Step 2: Install dependencies
pip install -r requirements.txt

ğŸ”¹ Step 3: Set your API key
Create a .env file inside the backend directory:

```TOGETHER_API_KEY=your_api_key_here
```

You can replace the Together API provider with your own LLM provider in main.py, or use a local model via Ollama if preferred.
The default LLM used is:

```model="meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"```

âš ï¸ Ensure your chosen model supports multimodal input.

ğŸ”¹ Step 4: Run the FastAPI server
    uvicorn main:app --reload



2. Frontend Setup (React with TypeScript)
cd frontend
npm install
npm run dev

This will start the frontend development server.


ğŸ§ª Features
Multimodal LLM support (text + image, etc.)

Customizable LLM provider (Together API, Ollama, etc.)

Frontend built with modern React and TypeScript

Simple .env-based configuration for API keys


ğŸ“Œ Notes
This app does not use a database at the moment â€” it's focused on demonstrating multimodal LLM capability via RAG.

Extend or modify the backendâ€™s main.py to customize your pipeline or LLM routing.

